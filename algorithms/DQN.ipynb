{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "import sys\n",
    "import logging # 日志记录用\n",
    "import gymnasium as gym\n",
    "import selfhealing_env\n",
    "from typing import Optional, Tuple, Union \n",
    "from stable_baselines3 import DQN\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logging.DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logger_obj(logger_name, level=logging.DEBUG, verbose=0):\n",
    "    # 就是一个日志记录器.可以放到utils里\n",
    "    \"\"\"\n",
    "    Method to return a custom logger with the given name and level\n",
    "    \"\"\"\n",
    "    logger = logging.getLogger(logger_name)\n",
    "    logger.setLevel(level)\n",
    "    format_string = (\"%(asctime)s - %(levelname)s - %(funcName)s (%(lineno)d):  %(message)s\")\n",
    "    datefmt = '%Y-%m-%d %I:%M:%S %p'\n",
    "    log_format = logging.Formatter(format_string, datefmt)\n",
    "\n",
    "    # Creating and adding the file handler\n",
    "    file_handler = logging.FileHandler(logger_name, mode='a')\n",
    "    file_handler.setFormatter(log_format)\n",
    "    logger.addHandler(file_handler)\n",
    "\n",
    "    if verbose == 1:\n",
    "        # Creating and adding the console handler\n",
    "        console_handler = logging.StreamHandler(sys.stdout)\n",
    "        console_handler.setFormatter(log_format)\n",
    "        logger.addHandler(console_handler)\n",
    "\n",
    "    return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainManager():\n",
    "    \n",
    "    def __init__(self,   \n",
    "                 env:gym.Env,\n",
    "                 log_output_path:Optional[str],\n",
    "                 log_level:int = logging.DEBUG,\n",
    "                 timer_enable: bool = False,\n",
    "                 episode_num:int = 1000,\n",
    "                 learning_rate: float = 0.001,\n",
    "                 buffer_size: int = 100000,\n",
    "                 learning_starts:int = 200,\n",
    "                 batch_size:int = 50,\n",
    "                 tau:float = 0.1,\n",
    "                 gamma:float = 0.95,\n",
    "                 exploration_final_eps:float = 0.05,\n",
    "                 verbose:int = 1,\n",
    "                 device:torch.device = torch.device(\"cpu\"),\n",
    "                 seed:Optional[int] = None\n",
    "                 ):\n",
    "        \n",
    "        self.env = env\n",
    "        self.device = device\n",
    "        self.seed = seed\n",
    "        self.episode_num = episode_num\n",
    "        self.time_steps = self.env.system_data.NT\n",
    "        self.timer_enable = timer_enable\n",
    "        \n",
    "        self.log_output_path = log_output_path\n",
    "        self.log_level = log_level\n",
    "        \n",
    "        self.Agent = DQN(env = self.env,\n",
    "                             learning_rate = learning_rate,\n",
    "                             buffer_size = buffer_size,\n",
    "                             learning_starts = learning_starts,\n",
    "                             batch_size = batch_size,\n",
    "                             tau = tau,\n",
    "                             gamma = gamma,\n",
    "                             exploration_final_eps = exploration_final_eps,\n",
    "                             verbose = verbose,\n",
    "                             device = self.device,\n",
    "                             seed = self.seed\n",
    "                             )\n",
    "        \n",
    "        self.set_loggers()\n",
    "                 \n",
    "        pass\n",
    "    \n",
    "    def set_loggers(self):\n",
    "        \n",
    "        now = datetime.now()\n",
    "        dt_string = now.strftime(\"__%Y_%m_%d_%H_%M\")\n",
    "        self.dt_string = dt_string\n",
    "        # check if the dir is given            \n",
    "        if self.log_output_path is None:\n",
    "            # if dir is not given, save results at root dir\n",
    "            output_path = os.getcwd()\n",
    "            log_output_path_name = output_path + '/' + \"log\" + dt_string + '.log'\n",
    "        else:\n",
    "            # if given, check if the saving directory exists\n",
    "            # if not given, create dir\n",
    "            if not os.path.isdir(self.log_output_path):\n",
    "                os.makedirs(self.log_output_path)\n",
    "            log_output_path_name = self.log_output_path + '/' + \"log\" + dt_string + '.log'\n",
    "            \n",
    "        self.logger = logger_obj(logger_name=log_output_path_name, level=self.log_level)  # log for debuging\n",
    "        # self.success_logger = SuccessLogger(ENV_NAME_2, output_path, title='Behavior Cloning')  # log for performance evaluation\n",
    "        \n",
    "    def test_agent(self):\n",
    "        test_options = {\"Specific_Disturbance\":None}\n",
    "        s0, _ = self.env.reset(options=test_options,seed=self.seed)\n",
    "        if self.timer_enable:\n",
    "            print(\"Begin solving benchmark\")\n",
    "            stime = time.time()\n",
    "            \n",
    "        # ================ calculate Benchmark value to normalize the restoration as ratio ==================\n",
    "        self.logger.info(\"-------------------Run_test begin--------------------\")\n",
    "        self.logger.info(\"The testing disturbance is {}\".format(self.env.disturbance))\n",
    "        # as a training benchmark, we only use tieline, which is the same action space with this agent\n",
    "        \n",
    "        \n",
    "        pass\n",
    "        \n",
    "    \n",
    "    def train(self):\n",
    "        \n",
    "        flag_convergence = False\n",
    "        tic = time.perf_counter()  # start clock\n",
    "        for idx_episode in range(self.episode_num):\n",
    "            if idx_episode % 10 == 0: # 控制episode打印频率\n",
    "                toc = time.perf_counter()\n",
    "                print(\"===================================================\")\n",
    "                print(f\"Training time: {toc - tic:0.4f} seconds; Mission {idx_episode:d} of {self.episode_num:d}\")\n",
    "                print(\"===================================================\")\n",
    "            self.logger.info(f\"=============== Mission {idx_episode:d} of {self.episode_num:d} =================\")\n",
    "            # executes the expert policy and perform Deep Q learning\n",
    "            self.agent.learn(total_timesteps=self.time_steps) # 这里5就是ENV中的NT\n",
    "            # test: execute learned policy on the environment\n",
    "            self.test_agent()\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
